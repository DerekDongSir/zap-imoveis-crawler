With the "largescalecrawlstuff.py" module you are able to crawl and parse Real Estate Market data from Brazil, from the site "zapimoveis.com"
The many intermediate datas let you be sure you will not lose your scraping, and in the raw data crawler you can stop and begin to crawl again and the crawling already done will be saved.
As ZapImoveis has much data available to crawl, and computers often crash when handling stuff of this magnitude in memory, I tried to make a solution to crawl from very little to very big quantities of data.

As ZapImoveis has a dinamic HTML, which is generated by javascript, we were only able to access their information using selenium webdriver, with a Chromiun Driver. 

The libs needed to use this modules are "re", "pandas", "bs4", "os" and "selenium".

We have 4 classes, whick we use to (i) generate a URL list to crawl and save it; (ii) crawl and save the raw data from the site; (iii) parse de real estate data into dicts and save it; and (iv) generate a csv with the crawled and parsed data

All the 'paths' we use here must exist, so be sure to create these folders
Once you crawled everithing you want, you just have to erase your folders with intermediate data or crate new ones and assign it

You can use these modules to even create your crawlers, adapt with the functions, and etc. I plan to do it sometime.
The folder for each intermediate data must be different and exist.

In order to explain it:
(i) To gather the url lists, we have the LargeScaleZapListMaker class
    1.To create an instance of it, you must give the paramenter:
    
    properties caracterictis:
    bairro='', - parameter to search (neighborhood)
    zona='', - parameter to search (city zone, as 'norte', 'sul', 'leste', and oeste)
    cidade='', - city
    estado='', -state

    as we use selenium with chrome, you must provide the path for your executable
    chromiun_path='/usr/lib/chromium-browser/chromedriver',

    batch_size=300 - if we have much data to mine, then we divide the url list in batches, so it easier to crawl each one. I suggest a batch size of 300, so puted it as default
    
    2. To get the url-to-crawl lists, you must execute this method:
    organize_and_export_url_list    
    with the parameter path, that is where the url list batches will be stored
    
(ii) To crawl the raw data and store it, we have the class LSperBatchMinifichaCrawler
    "Minificha" is a html tag used in the pages. I reference it in many variable names. 
    1. To create an instance of it, you must provide the following parameters:
    
    path="url_list" - path to the url-to-crawl lists. The lists must have been created by our listmaker class
    chromiun_path='/usr/lib/chromium-browser/chromedriver', 
    path_to_store='minifichas_list')
    
    2. To get and store the raw info, we have the "store_all_raw_html" method, for which you dont need to provide any parameters
    This will generate a corresponding JSON for each batch in the path_to_store parameter path.
    
(iii) To parse and gather the data we want from the raw data, we have the class LSperBatchMinifichaCrawler
    1. To create and instance of it, you must provide the parameters: 
    jsonpath="minifichas_list", - this is the path for the raw data
    to_store_path='parsed_fichas' - this is were you want to load the parsed data (json with a list of dicts, in which each dict contain the data for a property)
    
    2. To parse and store the data, you use the method parse_and_store_all()
    This will load the parsed data in the path_to_store path
    
(iv) To get a csv with the parsed data, you instance the class RealEstateDataCSVgen,
    1. Providing the parameters:
    parsed_json_path="parsed_fichas" - path to the parsed json
    csv_name='real_estate.csv' - the name of your csv file
    
    2. Once you use the create_csv method, the csv will be generated in the folder you ran the script. 
    
the "main.py" script has an example of use of this modules.
the urlListGenerator is a module used by the list maker to generate the url lists
feel free to explore the other functions, correct any bugs or use this software as you want and take the credit for it. 
    